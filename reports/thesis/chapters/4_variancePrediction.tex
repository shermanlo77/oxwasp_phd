In the previous chapter, it was found that it is very hard to fit a compound Poisson random variable onto the grey values directly itself. It is not totally useless however. The compound Poisson model showed that the variance of the grey value has a linear relationship with the mean grey value. Given this, it would be possible to predict the variance of the grey value, given the grey value. This opens up new ways to predict the uncertainty of each individual pixel in an x-ray scan.

In this chapter, different types of regressions were tested to see if they perform well in predicting grey value variance given the grey value. Two types of regressions were used. Generalised linear models \citep{nelder1972generalized} \citep{nelder1972generalized_2} \citep{mccullagh1984generalized} (GLM) is a parametric regression which models the response variable as a random variable in the exponential family. In this project, a Gamma distributed random variable was chosen as the response variable. A non-parametric regression was also considered, here there kernel regression \citep{friedman2001elements} was used.

Using the replicated x-ray images, the within individual pixel sample mean and sample variance were obtained. The sample variance-mean data were then used for the regressions to be fitted on. The deviance was investigated as a method of model selection. The regression was also used to test it's prediction performance on held out images.

In the end, a GLM with a linear relationship between the variance and the mean was good enough. Its performance is similar to other complicated models and its simple form attracts those who follows Occam's razor, that the simpler models are usually better. The linear relationship is also justified by the compound Poisson model.

\subsection{Generalised Linear Models}

The sample variance-mean data were obtained from the replicated projections. Let $x_{x,y}^{(r)}$ be the grey value of the pixel at $(x.y)$ from the $r$th replicate projection for $r=1,2,3,\dotdotdot,n$. The pixel sample mean is
\begin{equation}
    \overline{x}_{x,y}=\frac{1}{n}\sum_{r=1}^n x_{x,y}^{(r)}
\end{equation}
and the pixel sample variance is
\begin{equation}
    y_{x,y} =
    \frac{1}{n-1}
    \sum_{r=1}^n
        \left(
            x_{x,y}^{(r)} - \overline{x}_{x,y}
        \right)^2
    \ .
\end{equation}
Only pixels in the region of interest (ROI) were considered here, that is pixels which represent the test sample.

The aim is to model and predict the pixel variance given the pixel grey value by fitting a model onto the sample variance-mean data. No spatial information is used, thus the variance-mean data is denoted as $(x_1,y_1),(x_2,y_2),\cdots,(x_m,y_m)$ from now on, where $m$ is the area of the ROI or the size of the dataset. Let $Y(x)$ be a random variable and the sample variance given a grey value mean $x$. It was assumed the standard error from estimating the mean was negligible so that the uncertainty is captured by the random variable $Y$. It was assumed that for a given pixel, the grey values are Normally distributed and i.i.d. Let $\sigma^2(x)$ be the variance given a grey value $x$, then it can be shown that
\begin{equation}
\dfrac{(n-1)Y(x)}{\sigma^2(x)}\sim\chi^2_{n-1}
\end{equation}
which results in
\begin{equation}
Y(x)\sim\gammaDist\left(\alpha,\dfrac{\alpha}{\sigma^2(x)}\right)
\end{equation}
where $\alpha=(n-1)/2$ is the shape parameter. The expectation and variance are
\begin{equation}
\expectation\left[Y(x)\right]=\sigma^2(x)
\end{equation}
and
\begin{equation}
\variance\left[Y(x)\right]=\dfrac{\sigma^2(x)}{\alpha}
\end{equation}
respectively.

This framework allows the use of generalised linear models (GLM) \citep{nelder1972generalized,nelder1972generalized_2, mccullagh1984generalized}. In the gamma distribution case, GLM can be used to model
\begin{equation}
Y(x)\sim\gammaDist\left(\alpha,\dfrac{\alpha}{g^{-1}(\eta(x))}\right)
\end{equation}
where $g(\sigma^2)$ is the link function and $\eta(x)$ is a linear function called the systematic component. It should be noted that
\begin{equation}
  \expectation\left[Y(x)\right]=g^{-1}(\eta(x))
\end{equation}
which shows how the link function and systematic component work together. Examples of link functions are the identity link
\begin{equation}
g(\sigma^2)=g^{-1}(\sigma^2)=\sigma^2
\end{equation}
and, for the gamma distribution case, the canonical link
\begin{equation}
g(\sigma^2)=g^{-1}(\sigma^2)=1/\sigma^2 \ .
\end{equation}
An example of a systematic component are polynomials $\eta(x)=\beta_0+\sum_{j=1}^{p-1}\beta_j x^j$ so that when used with the identity link, for example, $\expectation\left[Y(x)\right] = \beta_0+\sum_{j=1}^{p-1}\beta_j x^j$. Iterative reweighted least squares \citep{friedman2001elements} can be used to estimate the parameters $\beta_0, \beta_1, \cdots, \beta_{p-1}$ given data for the model to fit onto.

Once the parameters have been estimated, prediction of the variance given a grey value $x$ is done using the expectation $\widehat{y}(x) = \expectation\left[Y(x)\right] = g^{-1}(\eta(x))$.

\section{Model Selection}

This section describes how forward stepwise selection \citep{friedman2001elements} was used to select which polynomials to use in the systematic component.

In summary, forward stepwise selection fits a basic model to the data initially. A term is added to make the model more complicated at each step to improve the fit onto the data. This is continued until the model cannot be improved subject to overfitting. The Akaike information criterion (AIC) \citep{friedman2001elements} and the Bayesian information criterion (BIC) \citep{friedman2001elements} are criteria which can be used to assess the fit of the model at each step without overfitting to the data too much.

The AIC and BIC are given as
\begin{equation}
\AIC = 2p-2\ln L
\end{equation}
and
\begin{equation}
\BIC = p\ln m - 2\ln L
\end{equation}
respectively where $p$ is the number of terms in the systematic component, $m$ is the size of the data set and $\ln L$ is the log likelihood of the GLM. The model with the lowest AIC or BIC is preferred. GLM aims to maximise the log likelihood but the additional terms in the criteria penalise models with too many terms. The log likelihood is given as
\begin{equation}
  \ln L = \sum_{i=1}^m \left[
    \alpha\ln\alpha
    -\ln\Gamma(\alpha)
    -\alpha\ln \widehat{y}_i
    +(\alpha-1)\ln y_i
    -\frac{\alpha y_i}{\widehat{y}_i}
  \right]
  \ .
\end{equation}
where $\widehat{y}_i=\expectation\left[Y(x_i)\right]$. It should be reminded that $\alpha=(n-1)/2$ was assumed to be known and does not need to be estimated.

The procedure is as follows. A criterion and a link function is chosen beforehand. In the initial step, a GLM with systematic component $\eta(x)=\beta_0$ is fitted and the criterion is recorded. In the next step, a polynomial term with one order higher is added to the systematic component $\eta(x)=\beta_0+\beta_1 x$, fitted and the criterion recorded. In addition, a polynomial term with one order lower is added $\beta(x)=\beta_{-1}x^{-1}+\beta_0$ and fitted separately with the criterion recorded. The model which decreases the criterion the most is accepted. Adding higher and lower order polynomials to the systematic component is repeated, for example after accepting $\eta(x)=\beta_0+\beta_1 x$, the following systematic components $\eta(x)=\beta_0+\beta_1 x+\beta_2x^2$ and $\eta(x)=\beta_{-1}x^{-1}+\beta_0+\beta_1 x$ are fitted and assessed. This is repeated until the criterion cannot be decreased and the procedure is left with the final model.

Forward stepwise selection was conducted on the datasets \texttt{AbsNoFilterDeg120} and \texttt{AbsFilterDeg120}. The procedure was repeated 100 times by using a random permutation with replacement of the replicated projections to work out a different sample variance-mean data which introduces some variation to the data. The procedure was also repeated using various shading corrections to investigate the effects of shading correction on the variance-mean relationship.

\begin{sidewaystable}
\centering
\begin{tabular}{cc|ccc}
\multicolumn{2}{c|}{Identity Link}& null & bw & linear \\ \hline
\multirow{3}{*}{AIC} & order     & \inputNumber{../figures/varmean/GlmSelectAicAbsNoFilterNull_identityorder.txt}     & \inputNumber{../figures/varmean/GlmSelectAicAbsNoFilterBw_identityorder.txt}     & \inputNumber{../figures/varmean/GlmSelectAicAbsNoFilterLinear_identityorder.txt}     \\
                     & votes     & \inputNumber{../figures/varmean/GlmSelectAicAbsNoFilterNull_identityvote.txt}      & \inputNumber{../figures/varmean/GlmSelectAicAbsNoFilterBw_identityvote.txt}      & \inputNumber{../figures/varmean/GlmSelectAicAbsNoFilterLinear_identityvote.txt}      \\
                     & criterion & \inputNumber{../figures/varmean/GlmSelectAicAbsNoFilterNull_identitycriterion.txt} & \inputNumber{../figures/varmean/GlmSelectAicAbsNoFilterBw_identitycriterion.txt} & \inputNumber{../figures/varmean/GlmSelectAicAbsNoFilterLinear_identitycriterion.txt} \\ \hline
\multirow{3}{*}{BIC} & order     & \inputNumber{../figures/varmean/GlmSelectBicAbsNoFilterNull_identityorder.txt}     & \inputNumber{../figures/varmean/GlmSelectBicAbsNoFilterBw_identityorder.txt}     & \inputNumber{../figures/varmean/GlmSelectBicAbsNoFilterLinear_identityorder.txt}     \\
                     & votes     & \inputNumber{../figures/varmean/GlmSelectBicAbsNoFilterNull_identityvote.txt}      & \inputNumber{../figures/varmean/GlmSelectBicAbsNoFilterBw_identityvote.txt}      & \inputNumber{../figures/varmean/GlmSelectBicAbsNoFilterLinear_identityvote.txt}      \\
                     & criterion & \inputNumber{../figures/varmean/GlmSelectBicAbsNoFilterNull_identitycriterion.txt} & \inputNumber{../figures/varmean/GlmSelectBicAbsNoFilterBw_identitycriterion.txt} & \inputNumber{../figures/varmean/GlmSelectBicAbsNoFilterLinear_identitycriterion.txt} \\
\\
\multicolumn{2}{c|}{Canonical Link}& null & bw & linear \\ \hline
\multirow{3}{*}{AIC} & order      & \inputNumber{../figures/varmean/GlmSelectAicAbsNoFilterNull_reciprocalorder.txt}     & \inputNumber{../figures/varmean/GlmSelectAicAbsNoFilterBw_reciprocalorder.txt}     & \inputNumber{../figures/varmean/GlmSelectAicAbsNoFilterLinear_reciprocalorder.txt}     \\
                     & votes      & \inputNumber{../figures/varmean/GlmSelectAicAbsNoFilterNull_reciprocalvote.txt}      & \inputNumber{../figures/varmean/GlmSelectAicAbsNoFilterBw_reciprocalvote.txt}      & \inputNumber{../figures/varmean/GlmSelectAicAbsNoFilterLinear_reciprocalvote.txt}      \\
                     & criterion  & \inputNumber{../figures/varmean/GlmSelectAicAbsNoFilterNull_reciprocalcriterion.txt} & \inputNumber{../figures/varmean/GlmSelectAicAbsNoFilterBw_reciprocalcriterion.txt} & \inputNumber{../figures/varmean/GlmSelectAicAbsNoFilterLinear_reciprocalcriterion.txt} \\ \hline
\multirow{3}{*}{BIC} & order      & \inputNumber{../figures/varmean/GlmSelectBicAbsNoFilterNull_reciprocalorder.txt}     & \inputNumber{../figures/varmean/GlmSelectBicAbsNoFilterBw_reciprocalorder.txt}     & \inputNumber{../figures/varmean/GlmSelectBicAbsNoFilterLinear_reciprocalorder.txt}     \\
                     & votes      & \inputNumber{../figures/varmean/GlmSelectBicAbsNoFilterNull_reciprocalvote.txt}      & \inputNumber{../figures/varmean/GlmSelectBicAbsNoFilterBw_reciprocalvote.txt}      & \inputNumber{../figures/varmean/GlmSelectBicAbsNoFilterLinear_reciprocalvote.txt}      \\
                     & criterion  & \inputNumber{../figures/varmean/GlmSelectBicAbsNoFilterNull_reciprocalcriterion.txt} & \inputNumber{../figures/varmean/GlmSelectBicAbsNoFilterBw_reciprocalcriterion.txt} & \inputNumber{../figures/varmean/GlmSelectBicAbsNoFilterLinear_reciprocalcriterion.txt}     
\end{tabular}
\caption{Forward stepwise selection was used to find suitable polynomial orders when fitting a GLM onto the sample variance-mean data of the grey values from the projections in \texttt{AbsNoFilterDeg120}. The columns of the table represent different shading corrections used on the projections. Forward stepwise selection was repeated 100 times by using a different random permutation with replacement to obtain a different sample variance-mean data. The row labelled order shows the most common selected polynomial orders. The error bars are the standard deviation from the 100 repeats.}
\label{table:meanVar_glmselect_absnofilter}
\end{sidewaystable}

\begin{sidewaystable}
\centering
\begin{tabular}{cc|ccc}
\multicolumn{2}{c|}{Identity Link}& null & bw & linear \\ \hline
\multirow{3}{*}{AIC} & order     & \inputNumber{../figures/varmean/GlmSelectAicAbsFilterNull_identityorder.txt}     & \inputNumber{../figures/varmean/GlmSelectAicAbsFilterBw_identityorder.txt}     & \inputNumber{../figures/varmean/GlmSelectAicAbsFilterLinear_identityorder.txt}     \\
                     & votes     & \inputNumber{../figures/varmean/GlmSelectAicAbsFilterNull_identityvote.txt}      & \inputNumber{../figures/varmean/GlmSelectAicAbsFilterBw_identityvote.txt}      & \inputNumber{../figures/varmean/GlmSelectAicAbsFilterLinear_identityvote.txt}      \\
                     & criterion & \inputNumber{../figures/varmean/GlmSelectAicAbsFilterNull_identitycriterion.txt} & \inputNumber{../figures/varmean/GlmSelectAicAbsFilterBw_identitycriterion.txt} & \inputNumber{../figures/varmean/GlmSelectAicAbsFilterLinear_identitycriterion.txt} \\ \hline
\multirow{3}{*}{BIC} & order     & \inputNumber{../figures/varmean/GlmSelectBicAbsFilterNull_identityorder.txt}     & \inputNumber{../figures/varmean/GlmSelectBicAbsFilterBw_identityorder.txt}     & \inputNumber{../figures/varmean/GlmSelectBicAbsFilterLinear_identityorder.txt}     \\
                     & votes     & \inputNumber{../figures/varmean/GlmSelectBicAbsFilterNull_identityvote.txt}      & \inputNumber{../figures/varmean/GlmSelectBicAbsFilterBw_identityvote.txt}      & \inputNumber{../figures/varmean/GlmSelectBicAbsFilterLinear_identityvote.txt}      \\
                     & criterion & \inputNumber{../figures/varmean/GlmSelectBicAbsFilterNull_identitycriterion.txt} & \inputNumber{../figures/varmean/GlmSelectBicAbsFilterBw_identitycriterion.txt} & \inputNumber{../figures/varmean/GlmSelectBicAbsFilterLinear_identitycriterion.txt} \\
\\
\multicolumn{2}{c|}{Canonical Link}& null & bw & linear \\ \hline
\multirow{3}{*}{AIC} & order      & \inputNumber{../figures/varmean/GlmSelectAicAbsFilterNull_reciprocalorder.txt}     & \inputNumber{../figures/varmean/GlmSelectAicAbsFilterBw_reciprocalorder.txt}     & \inputNumber{../figures/varmean/GlmSelectAicAbsFilterLinear_reciprocalorder.txt}     \\
                     & votes      & \inputNumber{../figures/varmean/GlmSelectAicAbsFilterNull_reciprocalvote.txt}      & \inputNumber{../figures/varmean/GlmSelectAicAbsFilterBw_reciprocalvote.txt}      & \inputNumber{../figures/varmean/GlmSelectAicAbsFilterLinear_reciprocalvote.txt}      \\
                     & criterion  & \inputNumber{../figures/varmean/GlmSelectAicAbsFilterNull_reciprocalcriterion.txt} & \inputNumber{../figures/varmean/GlmSelectAicAbsFilterBw_reciprocalcriterion.txt} & \inputNumber{../figures/varmean/GlmSelectAicAbsFilterLinear_reciprocalcriterion.txt} \\ \hline
\multirow{3}{*}{BIC} & order      & \inputNumber{../figures/varmean/GlmSelectBicAbsFilterNull_reciprocalorder.txt}     & \inputNumber{../figures/varmean/GlmSelectBicAbsFilterBw_reciprocalorder.txt}     & \inputNumber{../figures/varmean/GlmSelectBicAbsFilterLinear_reciprocalorder.txt}     \\
                     & votes      & \inputNumber{../figures/varmean/GlmSelectBicAbsFilterNull_reciprocalvote.txt}      & \inputNumber{../figures/varmean/GlmSelectBicAbsFilterBw_reciprocalvote.txt}      & \inputNumber{../figures/varmean/GlmSelectBicAbsFilterLinear_reciprocalvote.txt}      \\
                     & criterion  & \inputNumber{../figures/varmean/GlmSelectBicAbsFilterNull_reciprocalcriterion.txt} & \inputNumber{../figures/varmean/GlmSelectBicAbsFilterBw_reciprocalcriterion.txt} & \inputNumber{../figures/varmean/GlmSelectBicAbsFilterLinear_reciprocalcriterion.txt}     
\end{tabular}
\caption{Forward stepwise selection was used to find suitable polynomial orders when fitting a GLM onto the sample variance-mean data of the grey values from the projections in \texttt{AbsFilterDeg120}. The columns of the table represent different shading corrections used on the projections. Forward stepwise selection was repeated 100 times by using a different random permutation with replacement to obtain a different sample variance-mean data. The row labelled order shows the most common selected polynomial orders. The error bars are the standard deviation from the 100 repeats.}
\label{table:meanVar_glmselect_absfilter}
\end{sidewaystable}

The results are shown in Tables \ref{table:meanVar_glmselect_absnofilter} and \ref{table:meanVar_glmselect_absfilter}. The models selected all have only two terms and are quite simple. Different shading correction or different criteria had no effect on the selected model. The method is quite robust to the variation introduced to the dataset when repeating the experiment because all 100 repeats consistently selected the same model.

There was some variation to the selected models between datasets. For example when using the identity link, \texttt{AbsNoFilterDeg120} prefers $\eta(x)=\beta_0+\beta_1 x$ whereas \texttt{AbsFilterDeg120} prefers $\eta(x)=\beta_{-1}x^{-1}\beta_0$. Using the canonical link, both datasets selected $\eta(x)=\beta_{-1}x^{-1}\beta_0$ which correspond to $\widehat{y}(x)=\left(\beta_{-1}x^{-1}+\beta_0\right)^{-1}$. Figures \ref{fig:meanVar_varMeanExample_AbsNoFilterDeg120} and \ref{fig:meanVar_varMeanExample_AbsFilterDeg120} shows the GLM fits. There is a leverage towards the low grey values because there are more low grey values in the dataset. As a result, the GLM does not capture the curvature towards the high grey values.

\begin{figure}
  \centering
    \centerline{
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{../figures/varmean/varMeanExample_AbsNoFilterDeg120_identity000010.eps}
        \caption{$\widehat{y}(x)=\beta_0+\beta_1 x$}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{../figures/varmean/varMeanExample_AbsNoFilterDeg120_reciprocal000100.eps}
        \caption{$\widehat{y}(x)=(\beta_{-1}x^{-1}\beta_0)^{-1}$}
    \end{subfigure}
    }
    \caption{Log frequency density histogram of the sample variance-mean data from \texttt{AbsNoFilter}. The red solid line shows the GLM fit along with $\Phi(\pm 1)$ quantile error bars as dashed lines.}
    \label{fig:meanVar_varMeanExample_AbsNoFilterDeg120}
\end{figure}

\begin{figure}
  \centering
    \centerline{
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{../figures/varmean/varMeanExample_AbsFilterDeg120_identity000100.eps}
        \caption{$\widehat{y}(x)=\beta_{-1}x^{-1}+\beta_0$}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{../figures/varmean/varMeanExample_AbsFilterDeg120_reciprocal000100.eps}
        \caption{$\widehat{y}(x)=(\beta_{-1}x^{-1}+\beta_0)^{-1}$}
    \end{subfigure}
    }
    \caption{Log frequency density histogram of the sample variance-mean data from \texttt{AbsFilter}. The red solid line shows the GLM fit along with $\Phi(\pm 1)$ quantile error bars as dashed lines.}
    \label{fig:meanVar_varMeanExample_AbsFilterDeg120}
\end{figure}

\subsection{Deviance}
The deviance is one way to assess the goodness of fit of the regression on the variance-mean data. The deviance is defined to be
\begin{equation}
    D = 2\left(
        \ln L_s - \ln L    
    \right)
\end{equation}
where $L$ and $L_s$ are the likelihood and saturated likelihood respectively. 

Let $\alpha=(n-1)/2$ be the shape parameter. Suppose the variance-mean data consist of the following values $\{(y_1,x_1),(y_2,x_2),\dotdotdot,(y_m,x_m)\}$ and $\widehat{y}_i$ is the predicted $Y$ given $x_i$, then the log likelihood is

The log saturated likelihood is obtained by replacing all $\widehat{y}_i$ with $y_i$ so that
\begin{equation}
    \ln L_s = \sum_{i=1}^m \left[
        \alpha\ln\alpha
        -\alpha\ln{y_i}
        -\ln\Gamma(\alpha)
        +(\alpha-1)\ln y_i
        -\alpha
    \right]
    \ .
\end{equation}
Following from this, the deviance is then
\begin{equation}
    D = 2\alpha
    \sum_{i=1}^m\left[
        \dfrac{
            y_i-\widehat{y}_i
        }
        {
            \widehat{y}_i
        }
        - \ln\left(\dfrac{y_i}{\widehat{y}_i}\right)
    \right]
    \ .
\end{equation}
The scaled deviance can be obtained by removing the factor of $\alpha$ to get
\begin{equation}
    D^* = 2
    \sum_{i=1}^m\left[
        \dfrac{
            y_i-\widehat{y}_i
        }
        {
            \widehat{y}_i
        }
        - \ln\left(\dfrac{y_i}{\widehat{y}_i}\right)
    \right]
    \ .
\end{equation}
It is given that $D\sim\chi_{m-2}^2$ which implies that $\expectation\left[D\right]=m-2$. Because the GLMs considered here all have 2 parameters with known $\alpha$, the best model is the model with the lowest scaled deviance.

The analysis on the deviance was conducted by fitting GLMs with different link functions and polynomial orders $p$ onto the variance-mean data using all $n=100$ images from \texttt{Mar16}. The deviance was then calculated. This procedure was done using multiple bootstrap samples \citep{efron1992bootstrap} of the $m$ images to get a sample of scaled deviance.

The resulting 100 bootstrapped scaled deviance are shown in Figure \ref{fig:meanvar_deviance_mar16}. The kernel regression was also included. The scaled deviance does not vary much from model to model and suggests that the goodness of fit are all very similar.

\begin{figure}
    \centering
    \includegraphics[width=0.49\textwidth]{../figures/meanVar/GlmDeviance_Mar16_scaled_deviance.eps}
    \caption{Scaled deviance when fitting Gamma GLM on the variance-mean data from \texttt{Mar16}.}
    \label{fig:meanvar_deviance_mar16}
\end{figure}

In this particular example, $m\approx 1.3\times 10^6$ and $n=100$. Then one would expect the scaled deviance to have values
\begin{align*}
    \expectation\left[D^*\right]&=\frac{1}{\alpha}\expectation\left[D\right]
    \\
    &=\frac{2}{n-1}\times (m-2)
    \\
    &\approx \frac{2}{100-1}\times (1.3\times 10^6-2)
    \\
    &\approx 2.6 \times 10^4
\end{align*}
which is similar in magnitude but lower compared to the scaled deviances obtained from the bootstrap samples. This suggests the shape parameter $\alpha=(n-1)/2$ is too big and perhaps should be smaller. The estimation of the shape parameter here is however not too important as it is not used to further calculations.

\subsection{Cross Validation}

Another way to do model selection is cross validation \cite{friedman2001elements}. In the \texttt{Mar16} dataset, the 100 images were spilt into 2 equally sized sets called the training set and the test set. This produced 2 variance-mean data, one from each set. The regression was fitted onto the training set and the training set only.

Prediction using the fitted regression can be accessed. This was done by calculating the mean squared error of the prediction of the training set, known as the training error, or the test set, known as the test error.

20 samples of training and test errors were obtained by repeating the analysis using another random allocation of the training and test set. The results are shown in Figure \ref{fig:meanvar_mar16mse}. Each of the regressions considered all have similar prediction ability with the exception of GLM with log link and $p=1$ which performed significantly worse.

\begin{figure}
	\centering
    \centerline{
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{../figures/meanVar/GlmMse_Mar16_training_MSE.eps}
        \caption{Training error}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{../figures/meanVar/GlmMse_Mar16_test_MSE.eps}
        \caption{Test error}
    \end{subfigure}
    }
    \caption{Mean squared error in variance prediction using different regressions on the variance-mean data from \texttt{Mar16}.}
    \label{fig:meanvar_mar16mse}
\end{figure}

\subsection{Shading Correction}

%\begin{figure}
%	\centering
%    \centerline{
%    \begin{subfigure}[b]{0.49\textwidth}
%        \includegraphics[width=\textwidth]{../figures/meanVar/meanVarAbsBlock_July16_30deg.eps}
%        \caption{30 deg}
%    \end{subfigure}
%    \begin{subfigure}[b]{0.49\textwidth}
%        \includegraphics[width=\textwidth]{../figures/meanVar/meanVarAbsBlock_July16_120deg.eps}
%        \caption{120 deg}
%    \end{subfigure}
%    }
%    \caption{Log frequency density of the within pixel variance-mean data from \texttt{July16}. The error bars represent the variance-mean quartiles from the reference images.}
%\end{figure}
%
%\begin{figure}
%	\centering
%    \centerline{
%    \begin{subfigure}[b]{0.49\textwidth}
%        \includegraphics[width=\textwidth]{../figures/meanVar/meanVarAbsBlock_Sep16_30deg.eps}
%        \caption{30 deg}
%    \end{subfigure}
%    \begin{subfigure}[b]{0.49\textwidth}
%        \includegraphics[width=\textwidth]{../figures/meanVar/meanVarAbsBlock_Sep16_120deg.eps}
%        \caption{120 deg}
%    \end{subfigure}
%    }
%    \caption{Log frequency density of the within pixel variance-mean data from \texttt{Sep16}. The error bars represent the variance-mean quartiles from the reference images.}
%\end{figure}

It was investigated how shading correction affects the variance-mean relationship. Figure \ref{fig:meanvar_shadingcorrection_july16_30deg} shows the sample variance-mean for the \texttt{July16 30deg} data with different types of shading correction applied. At a quick glance, it appeared the shading correction did not do much to the sample variance-mean data. However a closer look at low means showed that shading correction had a particular effect and formed a tighter cluster in the bottom-left of the variance-mean histogram. This implied that shading correction made more dark pixels have similar grey values.

\begin{figure}
	\centering
    \centerline{
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{../figures/meanVar/MeanVarFit_July16_30deg_iShad1_iGlm1.eps}
        \caption{No shading correction}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{../figures/meanVar/MeanVarFit_July16_30deg_iShad2_iGlm1.eps}
        \caption{BW shading correction}
    \end{subfigure}
    }
    \centerline{
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{../figures/meanVar/MeanVarFit_July16_30deg_iShad3_iGlm1.eps}
        \caption{Linear shading correction}
    \end{subfigure}
    }
    \caption{Log frequency density of the within pixel variance-mean data from \texttt{July16 30deg} using different shading corrections. Fitted is a Gamma GLM with identity link. Error bars represent the $\Phi(\pm1)$ quantile.}
    \label{fig:meanvar_shadingcorrection_july16_30deg}
\end{figure}

In this section, the datasets \texttt{July16 30deg}, \texttt{July16 120deg}, \texttt{Sep16 30deg} and \texttt{Sep16 120deg} were considered. These are the only datasets with black, grey and white images which are used in shading correction.

A regression was fitted using all images in each dataset to get the sample variance-mean data. The scaled deviance was calculated and repeated using bootstrapped samples of the images to get 100 samples of the scaled deviance. These are shown in Figures \ref{fig:meanvar_deviance_july16} and \ref{fig:meanvar_deviance_sep16} for the datasets \texttt{July16} and \texttt{Sep16} respectively.

\begin{figure}
	\centering
    \centerline{
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{../figures/meanVar/GlmDeviance_July16_30deg_scaled_deviance.eps}
        \caption{30 deg}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{../figures/meanVar/GlmDeviance_July16_120deg_scaled_deviance.eps}
        \caption{120 deg}
    \end{subfigure}
    }
    \caption{Scaled deviance when fitting Gamma GLM on the within pixel variance-mean data from \texttt{July16}.}
    \label{fig:meanvar_deviance_july16}
\end{figure}

\begin{figure}
	\centering
    \centerline{
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{../figures/meanVar/GlmDeviance_Sep16_30deg_scaled_deviance.eps}
        \caption{30 deg}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{../figures/meanVar/GlmDeviance_Sep16_120deg_scaled_deviance.eps}
        \caption{120 deg}
    \end{subfigure}
    }
    \caption{Scaled deviance when fitting Gamma GLM on the within pixel variance-mean data from \texttt{Sep16}.}
    \label{fig:meanvar_deviance_sep16}
\end{figure}

The scaled deviance did not vary much from model to model and shading correction to another. However there was one notable exception in \texttt{July16 120deg} where it appeared the scaled deviance preferred the log link with $p=-2$. Figure \ref{fig:meanvar_july16_glm} shows how the curvature of the log link captured the sample variance-mean data.

\begin{figure}
	\centering
    \centerline{
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{../figures/meanVar/MeanVarFit_July16_120deg_iShad1_iGlm1.eps}
        \caption{Identity link, $p=1$}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{../figures/meanVar/MeanVarFit_July16_120deg_iShad1_iGlm3.eps}
        \caption{Log link, $p=-2$}
    \end{subfigure}
    }
    \caption{Log frequency density graph of the sample variance-mean data from the \texttt{July16 120deg} dataset.}
    \label{fig:meanvar_july16_glm}
\end{figure}

Worryingly there appeared to be a significant change in the scaled deviance at different angles in the \texttt{July 16} dataset. This is unexplained.

Another analysis was done by holding out half of the images and assigned it as the test set. The remaining images were assigned to be the training set and was used to train different types of regressions. The training and test MSE are shown in Figures \ref{fig:meanvar_mse_july16_30deg}, \ref{fig:meanvar_mse_july16_120deg}, \ref{fig:meanvar_mse_sep16_30deg} \& \ref{fig:meanvar_mse_sep16_120deg} for the datasets \texttt{July16 30deg}, \texttt{July16 120deg}, \texttt{Sep16 30deg} and \texttt{Sep16 120deg} respectively.

What is remarkable is how much shading correction improved the training and test MSE, in particular in the \texttt{Sep16} dataset.

\begin{figure}
\centering
    \centerline{
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{../figures/meanVar/GlmMse_July16_30deg_training_MSE.eps}
        \caption{Training error}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{../figures/meanVar/GlmMse_July16_30deg_test_MSE.eps}
        \caption{Test error}
    \end{subfigure}
    }
    \caption{Mean squared error in variance prediction using different regressions on the variance-mean data from \texttt{July16 30deg}.}
    \label{fig:meanvar_mse_july16_30deg}
\end{figure}

\begin{figure}
	\centering
    \centerline{
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{../figures/meanVar/GlmMse_July16_120deg_training_MSE.eps}
        \caption{Training error}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{../figures/meanVar/GlmMse_July16_120deg_test_MSE.eps}
        \caption{Test error}
    \end{subfigure}
    }
    \caption{Mean squared error in variance prediction using different regressions on the variance-mean data from \texttt{July16 120deg}.}
    \label{fig:meanvar_mse_july16_120deg}
\end{figure}

\begin{figure}
	\centering
    \centerline{
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{../figures/meanVar/GlmMse_Sep16_30deg_training_MSE.eps}
        \caption{Training error}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{../figures/meanVar/GlmMse_Sep16_30deg_test_MSE.eps}
        \caption{Test error}
    \end{subfigure}
    }
    \caption{Mean squared error in variance prediction using different regressions on the variance-mean data from \texttt{Sep16 30deg}.}
    \label{fig:meanvar_mse_sep16_30deg}
\end{figure}

\begin{figure}
	\centering
    \centerline{
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{../figures/meanVar/GlmMse_Sep16_120deg_training_MSE.eps}
        \caption{Training error}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{../figures/meanVar/GlmMse_Sep16_120deg_test_MSE.eps}
        \caption{Test error}
    \end{subfigure}
    }
    \caption{Mean squared error in variance prediction using different regressions on the variance-mean data from \texttt{Sep16 120deg}.}
    \label{fig:meanvar_mse_sep16_120deg}
\end{figure}


\section{Conclusion}
Experimentally it was found that a Gamma GLM with identity link and $p=1$ performed just as well with other GLM with different link functions and non-parametric regressions. A Gamma GLM with identity link and $p=1$ is essentially a simple linear regression. The simple linear regression is attractive as the linear relationship between the variance and mean is backed up by the compound Poisson model of x-ray photon behaviour. The simple linear regression is also a good candidate for the variance-mean model to those who prefer simpler models and follows Occamâ€™s razor.

It was also found that shading correction aid in variance prediction. This highlights the important of shading correction and should be applied to images whenever possible.